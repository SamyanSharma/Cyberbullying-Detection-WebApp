# Cyberbullying Detection WebApp

[![Project Status](https://img.shields.io/badge/Status-Active-success.svg)](https://github.com/Samyan1Sharma/Cyberbullying-Detection)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](https://github.com/Samyan1Sharma/Cyberbullying-Detection/blob/main/LICENSE)
[![Contributions Welcome](https://img.shields.io/badge/Contributions-Welcome-brightgreen.svg)](https://github.com/Samyan1Sharma/Cyberbullying-Detection/blob/main/CONTRIBUTING.md)
![Framework](https://img.shields.io/badge/Framework-Flask-blue)
![Model](https://img.shields.io/badge/Model-LinearSVC-brightgreen)
![Python](https://img.shields.io/badge/Language-Python-blue)

A WebApp that uses previously trained ML model for detecting toxic content in text. A Flask web application that detects cyberbullying in text using a tuned LinearSVC model (trained on [Cyberbullying Classification Dataset](https://www.kaggle.com/datasets/andrewmvd/cyberbullying-classification)).

## ðŸ“Œ Overview
This web application deploys a pre-trained LinearSVC model to identify potentially harmful content in real-time. While the model achieved competitive performance during testing, results should be interpreted as advisory (see Limitations).

![WebApp Interface](./WebApp_Interface.png)


## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details..

## Acknowledgments
- Thanks to all contributors who have helped make this project possible
- Special thanks to the open-source community for their invaluable tools and libraries
